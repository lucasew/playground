#!/usr/bin/env bash
# Start server with FP8 model on 4xH200
vllm serve zai-org/glm-4-9b-chat \
     --tensor-parallel-size 1 \
     --tool-call-parser glm47 \
     --reasoning-parser glm45 \
     --enable-auto-tool-choice \
     --trust-remote-code \
     --gpu-memory-utilization 0.90 \
     --enforce-eager \
     --quantization bitsandbytes \
     --load-format bitsandbytes
